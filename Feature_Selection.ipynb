{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Selection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMn3ok1eurk1hYFlrx88QjX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujitkumar205/RICEVD/blob/main/Feature_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial import distance\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_extraction.text  import CountVectorizer\n",
        "from sklearn.datasets import make_friedman1\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.svm import SVR\n",
        "from scipy.spatial import distance\n",
        "import statistics\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import f1_score\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from scipy.stats import gmean"
      ],
      "metadata": {
        "id": "SIX5WufmAr99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReliefF"
      ],
      "metadata": {
        "id": "yj6wCN2r8J88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reliefF(df, number_of_neighbours, instances_to_select, number_of_features):\n",
        "  features = df.iloc[:,:-1]\n",
        "  labels = df.iloc[:,-1]\n",
        "  rows,columns = features.shape\n",
        "\n",
        "  #initialize weights to zero\n",
        "  weights = np.zeros(columns,dtype = 'int')\n",
        "\n",
        "  #unique labels\n",
        "  unique_labels = np.unique(labels)\n",
        "\n",
        "  #used to select random instance\n",
        "  instances=np.array(list(range(1,rows)))\n",
        "\n",
        "  #difference between maximum and minimum of each feature used to calculate diff\n",
        "  minimums=np.min(features.values,axis=0)\n",
        "  maximums=np.max(features.values,axis=0)\n",
        "  difference=np.subtract(maximums,minimums)\n",
        "\n",
        "  for i in range(instances_to_select):\n",
        "\n",
        "    #choose a random instance and remove from instances to avoid selecting same thing again\n",
        "    random_instance = np.random.choice(instances[:-1])\n",
        "    instances=np.delete(instances,np.where(instances==random_instance))\n",
        "\n",
        "    #features of random instance used to calculate diff later on\n",
        "    random_instance_features = features.iloc[random_instance,:].values\n",
        "\n",
        "    #label of random instance and probability of label class\n",
        "    random_instance_label = labels[random_instance]\n",
        "    probability_random_instance_label = len(np.where(labels==random_instance_label)[0])/rows\n",
        "\n",
        "    #calculate euclidean distance between random instance and all other instances\n",
        "    distances = []\n",
        "    for temp in instances:\n",
        "      temp_features = features.iloc[temp,:].values\n",
        "      dist = distance.euclidean(random_instance_features,temp_features)\n",
        "      distances.append(dist)\n",
        "    \n",
        "    #sort instances based on distances\n",
        "    distances = np.array(distances)\n",
        "    arr1inds = distances.argsort()\n",
        "    sorted_distances = distances[arr1inds[::]]\n",
        "    sorted_instances = instances[arr1inds[::]]\n",
        "\n",
        "    #initialize list of nearest hits for random instance label and dictionary of nearest misses for every other label\n",
        "    nearest_hits = []\n",
        "    nearest_misses = {}\n",
        "\n",
        "    #finding nearest hits for random instance label\n",
        "    for temp in sorted_instances:\n",
        "      if labels[temp] == random_instance_label:\n",
        "        nearest_hits.append(temp)\n",
        "      if len(nearest_hits) == number_of_neighbours:\n",
        "        break\n",
        "      \n",
        "    #finding nearest misses for all other labels\n",
        "    for x in unique_labels:\n",
        "      if x == random_instance_label:\n",
        "        continue      \n",
        "      nearest_misses[x] = []\n",
        "      for temp in sorted_instances:\n",
        "        if labels[temp] == x:\n",
        "          nearest_misses[x].append(temp)\n",
        "        if len(nearest_misses[x]) == number_of_neighbours:\n",
        "          break\n",
        "\n",
        "    #used to find sum of diff function in weights equation for hits\n",
        "    total_hit = np.zeros(columns,dtype='int')\n",
        "\n",
        "    #find sum of diff function in weights equation for hits\n",
        "    for hit in range(len(nearest_hits)):\n",
        "      hI = features.iloc[nearest_hits[hit],:].values\n",
        "      dRH = np.divide(np.abs(np.subtract(random_instance_features,hI)),difference)\n",
        "      dRH = dRH/(instances_to_select * number_of_neighbours)\n",
        "      total_hit = np.add(total_hit,dRH)\n",
        "\n",
        "    #used to find sum of diff function in weights equation for misses\n",
        "    total_miss=np.zeros(columns,dtype='int')\n",
        "\n",
        "    #find sum of diff function in weights equation for misses in each class\n",
        "    for each_label in nearest_misses:\n",
        "      temp_miss=np.zeros(columns,dtype='int')\n",
        "      pclass=len(np.where(labels==each_label)[0])/rows #getting the probability of getting this class\n",
        "      postProb=pclass/(1-probability_random_instance_label) #calculating the posterior probability of getting this class\n",
        "\n",
        "      for each_miss in nearest_misses[each_label]:\n",
        "        mI = features.iloc[each_miss,:].values\n",
        "        dRM = np.divide(np.abs(np.subtract(random_instance_features,mI)),difference)\n",
        "        dRM = dRM/(instances_to_select * number_of_neighbours)\n",
        "        temp_miss = np.add(temp_miss,dRM)\n",
        "\n",
        "      total_miss = np.add(total_miss,(temp_miss*postProb))\n",
        "    \n",
        "    #update value of weights based on total hits and total miss and diff function values\n",
        "    weights=np.add(weights,total_miss)\n",
        "    weights=np.subtract(weights,total_hit) \n",
        "    \n",
        "\n",
        "  #select number_of_features weights with highest values and sort\n",
        "  ind = np.argpartition(weights, -number_of_features)[-number_of_features:]\n",
        "  ind = np.sort(ind)[::-1]\n",
        "\n",
        "  #column names of data frame\n",
        "  feature_names = list(df.columns.values)\n",
        "  feature_names = np.array(feature_names)\n",
        "\n",
        "  #top features based on weights\n",
        "  top_features = feature_names[ind]\n",
        "\n",
        "  return top_features\n"
      ],
      "metadata": {
        "id": "mzlhJsIlXT9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Chi-Square"
      ],
      "metadata": {
        "id": "FI8bHk_GAhEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chi_square(df, number_of_features):\n",
        "  features = df.iloc[:,:-1]\n",
        "  labels = df.iloc[:,-1]\n",
        "  labels=labels.astype('int') \n",
        "  test = SelectKBest(score_func=chi2, k=number_of_features)\n",
        "  fit = test.fit(features, labels)\n",
        "  chisquare_features = fit.get_feature_names_out(input_features=None)\n",
        "  return chisquare_features"
      ],
      "metadata": {
        "id": "VQvpZlenH-eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM-RFE"
      ],
      "metadata": {
        "id": "6lvGvnBFX2-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def svmrfe(df, number_of_features):\n",
        "  features = df.iloc[:,:-1]\n",
        "  labels = df.iloc[:,-1]\n",
        "  #estimator = SVR(kernel=\"linear\",cache_size=7000)\n",
        "  #estimator = LinearSVR(max_iter=100000,dual = True)\n",
        "  estimator = LinearSVC(random_state=0, tol=1e-5)\n",
        "  selector = RFE(estimator, n_features_to_select=number_of_features, step=10)\n",
        "  selector = selector.fit(features, labels)\n",
        "  feature_ranks = list(selector.ranking_)\n",
        "  feature_names = list(df.columns.values)\n",
        "  rank_dictionary = dict(zip(feature_names, feature_ranks))\n",
        "  svmrfe_features = [feature for feature, rank in rank_dictionary.items() if rank == 1]\n",
        "  return svmrfe_features\n"
      ],
      "metadata": {
        "id": "Nc5vrniyIvva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReliefF Variable Distance\n",
        "\n",
        "https://docs.scipy.org/doc/scipy/reference/spatial.distance.html"
      ],
      "metadata": {
        "id": "Adpmy9YZchUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcDistance(random_instance_features, temp_features, distance_variable):\n",
        "  if distance_variable == 0:\n",
        "    dist = distance.braycurtis(random_instance_features,temp_features)\n",
        "    return dist\n",
        "  if distance_variable == 1:\n",
        "    dist = distance.canberra(random_instance_features,temp_features)\n",
        "    return dist\n",
        "  if distance_variable == 2:\n",
        "    dist = distance.chebyshev(random_instance_features,temp_features)\n",
        "    return dist\n",
        "  if distance_variable == 3:\n",
        "    dist = distance.cityblock(random_instance_features,temp_features)\n",
        "    return dist\n",
        "  if distance_variable == 4:\n",
        "    dist = distance.correlation(random_instance_features,temp_features)\n",
        "    return dist\n",
        "  if distance_variable == 5:\n",
        "    dist = distance.cosine(random_instance_features,temp_features)\n",
        "    return dist\n",
        "  if distance_variable == 6:\n",
        "    dist = distance.euclidean(random_instance_features,temp_features)\n",
        "    return dist\n",
        "  if distance_variable == 7:\n",
        "    dist = distance.jensenshannon(random_instance_features,temp_features)\n",
        "    return dist\n",
        "  if distance_variable == 8:\n",
        "    dist = distance.sqeuclidean(random_instance_features,temp_features)\n",
        "    return dist\n",
        "  return 0"
      ],
      "metadata": {
        "id": "u8pkVtxAnO9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reliefF_variable(df, number_of_neighbours, instances_to_select, number_of_features, distance_variable):\n",
        "\n",
        "  features = df.iloc[:,:-1]\n",
        "  labels = df.iloc[:,-1]\n",
        "  rows,columns = features.shape\n",
        "\n",
        "  #initialize weights to zero\n",
        "  weights = np.zeros(columns,dtype = 'int')\n",
        "\n",
        "  #unique labels\n",
        "  unique_labels = np.unique(labels)\n",
        "\n",
        "  #used to select random instance\n",
        "  instances=np.array(list(range(1,rows)))\n",
        "\n",
        "  #difference between maximum and minimum of each feature used to calculate diff\n",
        "  minimums=np.min(features.values,axis=0)\n",
        "  maximums=np.max(features.values,axis=0)\n",
        "  difference=np.subtract(maximums,minimums)\n",
        "  \n",
        "  total_instances = instances_to_select * len(unique_labels)\n",
        "  label_count = {}\n",
        "\n",
        "  for i in unique_labels:\n",
        "    label_count[i] = 0\n",
        "\n",
        "  for i in range(total_instances):\n",
        "\n",
        "    #choose a random instance and remove from instances to avoid selecting same thing again\n",
        "    random_instance = np.random.choice(instances[:-1])\n",
        "    instances=np.delete(instances,np.where(instances==random_instance))\n",
        "\n",
        "    #features of random instance used to calculate diff later on\n",
        "    random_instance_features = features.iloc[random_instance,:].values\n",
        "\n",
        "    #label of random instance and probability of label class\n",
        "    random_instance_label = labels[random_instance]\n",
        "    probability_random_instance_label = len(np.where(labels==random_instance_label)[0])/rows\n",
        "\n",
        "    if label_count[random_instance_label] >= instances_to_select:\n",
        "      i = i-1\n",
        "      continue\n",
        "    \n",
        "    else:\n",
        "      label_count[random_instance_label] = label_count[random_instance_label] + 1\n",
        "\n",
        "    #calculate euclidean distance between random instance and all other instances\n",
        "    distances = []\n",
        "    for temp in instances:\n",
        "      temp_features = features.iloc[temp,:].values\n",
        "      dist = calcDistance(random_instance_features, temp_features, distance_variable)\n",
        "      distances.append(dist)\n",
        "    \n",
        "    #sort instances based on distances\n",
        "    distances = np.array(distances)\n",
        "    arr1inds = distances.argsort()\n",
        "    sorted_distances = distances[arr1inds[::]]\n",
        "    sorted_instances = instances[arr1inds[::]]\n",
        "\n",
        "    #initialize list of nearest hits for random instance label and dictionary of nearest misses for every other label\n",
        "    nearest_hits = []\n",
        "    nearest_misses = {}\n",
        "\n",
        "    #finding nearest hits for random instance label\n",
        "    for temp in sorted_instances:\n",
        "      if labels[temp] == random_instance_label:\n",
        "        nearest_hits.append(temp)\n",
        "      if len(nearest_hits) == number_of_neighbours:\n",
        "        break\n",
        "      \n",
        "    #finding nearest misses for all other labels\n",
        "    for x in unique_labels:\n",
        "      if x == random_instance_label:\n",
        "        continue      \n",
        "      nearest_misses[x] = []\n",
        "      for temp in sorted_instances:\n",
        "        if labels[temp] == x:\n",
        "          nearest_misses[x].append(temp)\n",
        "        if len(nearest_misses[x]) == number_of_neighbours:\n",
        "          break\n",
        "\n",
        "    #used to find sum of diff function in weights equation for hits\n",
        "    total_hit = np.zeros(columns,dtype='int')\n",
        "\n",
        "    #find sum of diff function in weights equation for hits\n",
        "    for hit in range(len(nearest_hits)):\n",
        "      hI = features.iloc[nearest_hits[hit],:].values\n",
        "      dRH = np.divide(np.abs(np.subtract(random_instance_features,hI)),difference)\n",
        "      dRH = dRH/(instances_to_select * number_of_neighbours)\n",
        "      total_hit = np.add(total_hit,dRH)\n",
        "\n",
        "    #used to find sum of diff function in weights equation for misses\n",
        "    total_miss=np.zeros(columns,dtype='int')\n",
        "\n",
        "    #find sum of diff function in weights equation for misses in each class\n",
        "    for each_label in nearest_misses:\n",
        "      temp_miss=np.zeros(columns,dtype='int')\n",
        "      pclass=len(np.where(labels==each_label)[0])/rows #getting the probability of getting this class\n",
        "      postProb=pclass/(1-probability_random_instance_label) #calculating the posterior probability of getting this class\n",
        "\n",
        "      for each_miss in nearest_misses[each_label]:\n",
        "        mI = features.iloc[each_miss,:].values\n",
        "        dRM = np.divide(np.abs(np.subtract(random_instance_features,mI)),difference)\n",
        "        dRM = dRM/(instances_to_select * number_of_neighbours)\n",
        "        temp_miss = np.add(temp_miss,dRM)\n",
        "\n",
        "      total_miss = np.add(total_miss,(temp_miss*postProb))\n",
        "    \n",
        "    #update value of weights based on total hits and total miss and diff function values\n",
        "    weights=np.add(weights,total_miss)\n",
        "    weights=np.subtract(weights,total_hit) \n",
        "    \n",
        "\n",
        "  #select number_of_features weights with highest values and sort\n",
        "  ind = np.argpartition(weights, -number_of_features)[-number_of_features:]\n",
        "  ind = np.sort(ind)[::-1]\n",
        "\n",
        "  #column names of data frame\n",
        "  feature_names = list(df.columns.values)\n",
        "  feature_names = np.array(feature_names)\n",
        "\n",
        "  #top features based on weights\n",
        "  top_features = feature_names[ind]\n",
        "\n",
        "  return top_features\n"
      ],
      "metadata": {
        "id": "6mbgNO1WchUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reliefF_variable_main(df, number_of_neighbours, instances_to_select, number_of_features):\n",
        "\n",
        "  features = df.iloc[:,:-1]\n",
        "\n",
        "  braycurtis = []\n",
        "  canberra = []\n",
        "  chebyshev = []\n",
        "  cityblock = []\n",
        "  correlation = []\n",
        "  cosine = []\n",
        "  euclidean = []\n",
        "  jensenshannon = []\n",
        "  sqeuclidean = []\n",
        "\n",
        "  for i in range(len(features)-1):\n",
        "    for j in range(i+1, len(features)):\n",
        "      braycurtis.append(distance.braycurtis(features.iloc[i],features.iloc[j]))\n",
        "      canberra.append(distance.canberra(features.iloc[i],features.iloc[j]))\n",
        "      chebyshev.append(distance.chebyshev(features.iloc[i],features.iloc[j]))\n",
        "      cityblock.append(distance.cityblock(features.iloc[i],features.iloc[j]))\n",
        "      correlation.append(distance.correlation(features.iloc[i],features.iloc[j]))\n",
        "      cosine.append(distance.cosine(features.iloc[i],features.iloc[j]))\n",
        "      euclidean.append(distance.euclidean(features.iloc[i],features.iloc[j]))\n",
        "      jensenshannon.append(distance.jensenshannon(features.iloc[i],features.iloc[j]))\n",
        "      sqeuclidean.append(distance.sqeuclidean(features.iloc[i],features.iloc[j]))\n",
        "\n",
        "  standard_deviation = []\n",
        "  standard_deviation.append(statistics.stdev(braycurtis))\n",
        "  standard_deviation.append(statistics.stdev(canberra))\n",
        "  standard_deviation.append(statistics.stdev(chebyshev))\n",
        "  standard_deviation.append(statistics.stdev(cityblock))\n",
        "  standard_deviation.append(statistics.stdev(correlation))\n",
        "  standard_deviation.append(statistics.stdev(cosine))\n",
        "  standard_deviation.append(statistics.stdev(euclidean))\n",
        "  standard_deviation.append(statistics.stdev(jensenshannon))\n",
        "  standard_deviation.append(statistics.stdev(sqeuclidean))\n",
        "\n",
        "  max_value = max(standard_deviation)\n",
        "  distance_variable = standard_deviation.index(max_value)\n",
        "\n",
        "  print(distance_variable)\n",
        "  #distance_variable = 1\n",
        "  features_combined = []\n",
        "\n",
        "  for i in range(10):\n",
        "    a = reliefF_variable(df, number_of_neighbours, instances_to_select, number_of_features, distance_variable)\n",
        "    features_combined = features_combined + list(a)\n",
        "\n",
        "  features_count = Counter(features_combined)\n",
        "\n",
        "  features_count_sorted = sorted(features_count.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  variable_distance_relieff_features = []\n",
        "  for i in range(number_of_features):\n",
        "    variable_distance_relieff_features.append(features_count_sorted[i][0])\n",
        "\n",
        "  return variable_distance_relieff_features"
      ],
      "metadata": {
        "id": "uVClRo0xfWI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing Feature Selection Algorithms"
      ],
      "metadata": {
        "id": "ZPfOhCjk2YG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Control Function"
      ],
      "metadata": {
        "id": "jn2FuSc4ynAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def normalize_data(dataframe):\n",
        "  dataframe=(dataframe-dataframe.mean())/dataframe.std()\n",
        "  dataframe=(dataframe-dataframe.min())/(dataframe.max()-dataframe.min())\n",
        "  dataframe[np.isnan(dataframe)] = 0\n",
        "  return dataframe"
      ],
      "metadata": {
        "id": "INFtxUdEDblS",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def plot_roc_auc(x_test, y_test, model):\n",
        "  #define metrics\n",
        "  y_pred_proba = model.predict_proba(x_test)[::,1]\n",
        "  y_pred_proba = normalize_data(y_pred_proba)\n",
        "  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "  auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "  rocauc_score = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "  #print('The ROCAUC is {}'.format(rocauc_score))\n",
        "\n",
        "\n",
        "  #create ROC curve\n",
        "  #plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
        "  #plt.ylabel('True Positive Rate')\n",
        "  #plt.xlabel('False Positive Rate')\n",
        "  #plt.legend(loc=4)\n",
        "  #plt.show()\n",
        "\n",
        "  return rocauc_score"
      ],
      "metadata": {
        "id": "eyzkbPuL44As",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def training_model(train, test, fold_no, model):\n",
        "  x_train = train.drop(['label'],axis=1)\n",
        "  y_train = train.label\n",
        "  x_test = test.drop(['label'],axis=1)\n",
        "  y_test = test.label\n",
        "  model.fit(x_train, y_train)\n",
        "  score = model.score(x_test,y_test)\n",
        "  scores_list = []\n",
        "\n",
        "  #print('For Fold {} the accuracy is {}'.format(str(fold_no),score))\n",
        "  scores_list.append(score)\n",
        "\n",
        "  f1_scores = f1_score(y_test,model.predict(x_test))\n",
        "  #print('F1 score is {}'.format(f1_scores))\n",
        "  scores_list.append(f1_scores)\n",
        "\n",
        "  unique_labels = np.unique(y_test)\n",
        "  predicted = model.predict(x_test)\n",
        "  #y_test = list(y_test)\n",
        "  #print(y_test)\n",
        "\n",
        "  '''\n",
        "\n",
        "  recall = []\n",
        "  for k in unique_labels:\n",
        "    indices = [int(i) for i, x in enumerate(y_test) if x == k]\n",
        "    res_list_pred = list(map(predicted.__getitem__, indices))\n",
        "    res_list_test = list(map(list(y_test).__getitem__, indices))\n",
        "    recall_class = recall_score(res_list_pred, res_list_test)\n",
        "    recall.append(recall_class)\n",
        "  \n",
        "  recall = [0.000001 if x == 0 else x for x in recall]\n",
        "  geometric_mean = gmean(recall)\n",
        "\n",
        "  '''\n",
        "\n",
        "\n",
        "  geometric_mean = geometric_mean_score(y_test,model.predict(x_test))\n",
        "  #print('The geometric mean is {}'.format(geometric_mean))\n",
        "  scores_list.append(geometric_mean)\n",
        "\n",
        "  roc_scores = plot_roc_auc(x_test, y_test,model)\n",
        "  scores_list.append(roc_scores)\n",
        "\n",
        "  return scores_list"
      ],
      "metadata": {
        "cellView": "code",
        "id": "YIxUaDRqx-UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def control_function(dataframe, n_of_splits, df_name, path_name, df_scores):\n",
        "  #dataframe = dataframe.reset_index()\n",
        "  #dataframe = dataframe.replace(np.inf, dataframe.mean())\n",
        "  #dataframe = dataframe.fillna(dataframe.mean())\n",
        "  skf = StratifiedKFold(n_splits=n_of_splits)\n",
        "  x = dataframe\n",
        "  y = dataframe.label\n",
        "\n",
        "  svc_model = SVC(kernel='linear',probability=True)\n",
        "  dtc_model = DecisionTreeClassifier(random_state = 0)\n",
        "  rfc_model = RandomForestClassifier(random_state=0)\n",
        "  naive_bayes_model = GaussianNB()\n",
        "  gradient_boosting_model = GradientBoostingClassifier(random_state=0)\n",
        "  knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "  models = [svc_model, dtc_model, rfc_model, naive_bayes_model, gradient_boosting_model, knn_model]\n",
        "  models_name = ['Support Vector Classifier', 'Decision Tree Classifier', 'Random Forest Classifier', 'Gaussian Naive Bayes', 'Gradient Boosting Classifier', 'K Nearest Neighbour']\n",
        "\n",
        "\n",
        "  for i in range(len(models)):\n",
        "    print(models_name[i], \" + \", df_name, \" + \", path_name, \"\\n\")    \n",
        "    fold_no = 1\n",
        "    for train_index,test_index in skf.split(x, y):\n",
        "      train = dataframe.iloc[train_index,:]\n",
        "      test = dataframe.iloc[test_index,:]\n",
        "      model_scores = training_model(train, test, fold_no, models[i])\n",
        "      df_scores.loc[0 if pd.isnull(df_scores.index.max()) else df_scores.index.max() + 1] = [df_name, path_name, models_name[i], fold_no] + model_scores\n",
        "      fold_no += 1\n",
        "      #print(\"\\n\")"
      ],
      "metadata": {
        "id": "O8Uqn8__ywX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_dataframe(dataframe):\n",
        "  dataframe=(dataframe-dataframe.mean())/dataframe.std()\n",
        "  dataframe=(dataframe-dataframe.min())/(dataframe.max()-dataframe.min())\n",
        "  dataframe.fillna(0,inplace=True)\n",
        "  return dataframe"
      ],
      "metadata": {
        "id": "JqLR4zTMAvAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_function(path_name, df_scores, feature_names_path, count):\n",
        "  df = pd.read_csv(path_name)\n",
        "\n",
        "  print(df.shape)\n",
        "\n",
        "  number_of_neighbours = 5\n",
        "  instances_to_select = 10\n",
        "  number_of_features = 100\n",
        "\n",
        "  \n",
        "  df = normalize_dataframe(df)\n",
        "\n",
        "  #Chi-Square\n",
        "  chisquare_features = chi_square(df, number_of_features)\n",
        "  df_chisquare = pd.read_csv(path_name, usecols = chisquare_features)\n",
        "  df_chisquare = normalize_dataframe(df_chisquare)\n",
        "  \n",
        "\n",
        "  #ReliefF\n",
        "  reliefF_features = reliefF(df, number_of_neighbours, instances_to_select, number_of_features)\n",
        "  df_reliefF = pd.read_csv(path_name, usecols = reliefF_features)\n",
        "  df_reliefF = normalize_dataframe(df_reliefF)\n",
        "  \n",
        "\n",
        "  #SVM-RFE\n",
        "  svmrfe_features = svmrfe(df, number_of_features)\n",
        "  df_svmrfe = pd.read_csv(path_name, usecols = svmrfe_features)\n",
        "  df_svmrfe = normalize_dataframe(df_svmrfe)\n",
        "  \n",
        "\n",
        "  #Variable-ReliefF\n",
        "  variable_distance_relieff_features = reliefF_variable_main(df, number_of_neighbours, instances_to_select, number_of_features)\n",
        "  df_variable_reliefF = pd.read_csv(path_name, usecols = variable_distance_relieff_features)\n",
        "  df_variable_reliefF = normalize_dataframe(df_variable_reliefF)\n",
        "  \n",
        "\n",
        "\n",
        "  #Original\n",
        "  df_original = pd.read_csv(path_name).iloc[:,:-1]\n",
        "  df_original = normalize_dataframe(df_original)\n",
        "\n",
        "  features = pd.DataFrame(\n",
        "    {'svmrfe': svmrfe_features,\n",
        "     'chisquare': chisquare_features,\n",
        "     'relief': reliefF_features,\n",
        "     'variable': variable_distance_relieff_features\n",
        "    })\n",
        "  feature_names_path = feature_names_path + str(count)\n",
        "  features.to_csv(feature_names_path)\n",
        "\n",
        "\n",
        "  #Labels\n",
        "  df_labels = pd.read_csv(path_name).iloc[:,-1]\n",
        "  df_original['label'] = df_labels\n",
        "  df_reliefF['label'] = df_labels\n",
        "  df_chisquare['label'] = df_labels\n",
        "  df_svmrfe['label'] = df_labels\n",
        "  df_variable_reliefF['label'] = df_labels\n",
        "\n",
        "  n_of_splits = 5\n",
        "\n",
        "  df_list = [df_original, df_reliefF, df_chisquare, df_svmrfe, df_variable_reliefF]\n",
        "  df_list_name = ['Original', 'ReliefF', 'ChiSquare', 'SVMRFE', 'Variable ReliefF']\n",
        "\n",
        "  for df_index in range(len(df_list)):\n",
        "    control_function(df_list[df_index], n_of_splits, df_list_name[df_index], path_name, df_scores)"
      ],
      "metadata": {
        "id": "w0ZOMXBS1Uc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_name_list = ['/content/gastroenterology.csv','/content/leukemia.csv', '/content/colon 2000.csv', '/content/DLBCL.csv', '/content/LSVT_voice_rehabilitation.csv', '/content/gastric cancer.csv',]\n",
        "#path_name_list = ['/content/staDynBenignLab.csv','/content/qsar_androgen_receptor.csv','/content/qsar_oral_toxicity.csv']\n",
        "df_scores = pd.DataFrame(columns=['Algorithm','Dataset','Model','Fold Number', 'Accuracy','F1','Geometric Mean','AUC'])\n",
        "feature_names_path = 'featuresLow.csv'\n",
        "count = 1\n",
        "for path_name in path_name_list:\n",
        "  main_function(path_name, df_scores,feature_names_path, count)\n",
        "  count = count + 1"
      ],
      "metadata": {
        "id": "5d7SC1NisLNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1d7b08-059d-4517-c535-549800a79317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(152, 699)\n",
            "Support Vector Classifier  +  Original  +  /content/gastroenterology.csv \n",
            "\n",
            "Decision Tree Classifier  +  Original  +  /content/gastroenterology.csv \n",
            "\n",
            "Random Forest Classifier  +  Original  +  /content/gastroenterology.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  Original  +  /content/gastroenterology.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  Original  +  /content/gastroenterology.csv \n",
            "\n",
            "K Nearest Neighbour  +  Original  +  /content/gastroenterology.csv \n",
            "\n",
            "Support Vector Classifier  +  ReliefF  +  /content/gastroenterology.csv \n",
            "\n",
            "Decision Tree Classifier  +  ReliefF  +  /content/gastroenterology.csv \n",
            "\n",
            "Random Forest Classifier  +  ReliefF  +  /content/gastroenterology.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  ReliefF  +  /content/gastroenterology.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  ReliefF  +  /content/gastroenterology.csv \n",
            "\n",
            "K Nearest Neighbour  +  ReliefF  +  /content/gastroenterology.csv \n",
            "\n",
            "Support Vector Classifier  +  ChiSquare  +  /content/gastroenterology.csv \n",
            "\n",
            "Decision Tree Classifier  +  ChiSquare  +  /content/gastroenterology.csv \n",
            "\n",
            "Random Forest Classifier  +  ChiSquare  +  /content/gastroenterology.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  ChiSquare  +  /content/gastroenterology.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  ChiSquare  +  /content/gastroenterology.csv \n",
            "\n",
            "K Nearest Neighbour  +  ChiSquare  +  /content/gastroenterology.csv \n",
            "\n",
            "Support Vector Classifier  +  SVMRFE  +  /content/gastroenterology.csv \n",
            "\n",
            "Decision Tree Classifier  +  SVMRFE  +  /content/gastroenterology.csv \n",
            "\n",
            "Random Forest Classifier  +  SVMRFE  +  /content/gastroenterology.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  SVMRFE  +  /content/gastroenterology.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  SVMRFE  +  /content/gastroenterology.csv \n",
            "\n",
            "K Nearest Neighbour  +  SVMRFE  +  /content/gastroenterology.csv \n",
            "\n",
            "Support Vector Classifier  +  Variable ReliefF  +  /content/gastroenterology.csv \n",
            "\n",
            "Decision Tree Classifier  +  Variable ReliefF  +  /content/gastroenterology.csv \n",
            "\n",
            "Random Forest Classifier  +  Variable ReliefF  +  /content/gastroenterology.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  Variable ReliefF  +  /content/gastroenterology.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  Variable ReliefF  +  /content/gastroenterology.csv \n",
            "\n",
            "K Nearest Neighbour  +  Variable ReliefF  +  /content/gastroenterology.csv \n",
            "\n",
            "(72, 5148)\n",
            "Support Vector Classifier  +  Original  +  /content/leukemia.csv \n",
            "\n",
            "Decision Tree Classifier  +  Original  +  /content/leukemia.csv \n",
            "\n",
            "Random Forest Classifier  +  Original  +  /content/leukemia.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  Original  +  /content/leukemia.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  Original  +  /content/leukemia.csv \n",
            "\n",
            "K Nearest Neighbour  +  Original  +  /content/leukemia.csv \n",
            "\n",
            "Support Vector Classifier  +  ReliefF  +  /content/leukemia.csv \n",
            "\n",
            "Decision Tree Classifier  +  ReliefF  +  /content/leukemia.csv \n",
            "\n",
            "Random Forest Classifier  +  ReliefF  +  /content/leukemia.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  ReliefF  +  /content/leukemia.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  ReliefF  +  /content/leukemia.csv \n",
            "\n",
            "K Nearest Neighbour  +  ReliefF  +  /content/leukemia.csv \n",
            "\n",
            "Support Vector Classifier  +  ChiSquare  +  /content/leukemia.csv \n",
            "\n",
            "Decision Tree Classifier  +  ChiSquare  +  /content/leukemia.csv \n",
            "\n",
            "Random Forest Classifier  +  ChiSquare  +  /content/leukemia.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  ChiSquare  +  /content/leukemia.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  ChiSquare  +  /content/leukemia.csv \n",
            "\n",
            "K Nearest Neighbour  +  ChiSquare  +  /content/leukemia.csv \n",
            "\n",
            "Support Vector Classifier  +  SVMRFE  +  /content/leukemia.csv \n",
            "\n",
            "Decision Tree Classifier  +  SVMRFE  +  /content/leukemia.csv \n",
            "\n",
            "Random Forest Classifier  +  SVMRFE  +  /content/leukemia.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  SVMRFE  +  /content/leukemia.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  SVMRFE  +  /content/leukemia.csv \n",
            "\n",
            "K Nearest Neighbour  +  SVMRFE  +  /content/leukemia.csv \n",
            "\n",
            "Support Vector Classifier  +  Variable ReliefF  +  /content/leukemia.csv \n",
            "\n",
            "Decision Tree Classifier  +  Variable ReliefF  +  /content/leukemia.csv \n",
            "\n",
            "Random Forest Classifier  +  Variable ReliefF  +  /content/leukemia.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  Variable ReliefF  +  /content/leukemia.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  Variable ReliefF  +  /content/leukemia.csv \n",
            "\n",
            "K Nearest Neighbour  +  Variable ReliefF  +  /content/leukemia.csv \n",
            "\n",
            "(62, 2001)\n",
            "Support Vector Classifier  +  Original  +  /content/colon 2000.csv \n",
            "\n",
            "Decision Tree Classifier  +  Original  +  /content/colon 2000.csv \n",
            "\n",
            "Random Forest Classifier  +  Original  +  /content/colon 2000.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  Original  +  /content/colon 2000.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  Original  +  /content/colon 2000.csv \n",
            "\n",
            "K Nearest Neighbour  +  Original  +  /content/colon 2000.csv \n",
            "\n",
            "Support Vector Classifier  +  ReliefF  +  /content/colon 2000.csv \n",
            "\n",
            "Decision Tree Classifier  +  ReliefF  +  /content/colon 2000.csv \n",
            "\n",
            "Random Forest Classifier  +  ReliefF  +  /content/colon 2000.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  ReliefF  +  /content/colon 2000.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  ReliefF  +  /content/colon 2000.csv \n",
            "\n",
            "K Nearest Neighbour  +  ReliefF  +  /content/colon 2000.csv \n",
            "\n",
            "Support Vector Classifier  +  ChiSquare  +  /content/colon 2000.csv \n",
            "\n",
            "Decision Tree Classifier  +  ChiSquare  +  /content/colon 2000.csv \n",
            "\n",
            "Random Forest Classifier  +  ChiSquare  +  /content/colon 2000.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  ChiSquare  +  /content/colon 2000.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  ChiSquare  +  /content/colon 2000.csv \n",
            "\n",
            "K Nearest Neighbour  +  ChiSquare  +  /content/colon 2000.csv \n",
            "\n",
            "Support Vector Classifier  +  SVMRFE  +  /content/colon 2000.csv \n",
            "\n",
            "Decision Tree Classifier  +  SVMRFE  +  /content/colon 2000.csv \n",
            "\n",
            "Random Forest Classifier  +  SVMRFE  +  /content/colon 2000.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  SVMRFE  +  /content/colon 2000.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  SVMRFE  +  /content/colon 2000.csv \n",
            "\n",
            "K Nearest Neighbour  +  SVMRFE  +  /content/colon 2000.csv \n",
            "\n",
            "Support Vector Classifier  +  Variable ReliefF  +  /content/colon 2000.csv \n",
            "\n",
            "Decision Tree Classifier  +  Variable ReliefF  +  /content/colon 2000.csv \n",
            "\n",
            "Random Forest Classifier  +  Variable ReliefF  +  /content/colon 2000.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  Variable ReliefF  +  /content/colon 2000.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  Variable ReliefF  +  /content/colon 2000.csv \n",
            "\n",
            "K Nearest Neighbour  +  Variable ReliefF  +  /content/colon 2000.csv \n",
            "\n",
            "(77, 7071)\n",
            "Support Vector Classifier  +  Original  +  /content/DLBCL.csv \n",
            "\n",
            "Decision Tree Classifier  +  Original  +  /content/DLBCL.csv \n",
            "\n",
            "Random Forest Classifier  +  Original  +  /content/DLBCL.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  Original  +  /content/DLBCL.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  Original  +  /content/DLBCL.csv \n",
            "\n",
            "K Nearest Neighbour  +  Original  +  /content/DLBCL.csv \n",
            "\n",
            "Support Vector Classifier  +  ReliefF  +  /content/DLBCL.csv \n",
            "\n",
            "Decision Tree Classifier  +  ReliefF  +  /content/DLBCL.csv \n",
            "\n",
            "Random Forest Classifier  +  ReliefF  +  /content/DLBCL.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  ReliefF  +  /content/DLBCL.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  ReliefF  +  /content/DLBCL.csv \n",
            "\n",
            "K Nearest Neighbour  +  ReliefF  +  /content/DLBCL.csv \n",
            "\n",
            "Support Vector Classifier  +  ChiSquare  +  /content/DLBCL.csv \n",
            "\n",
            "Decision Tree Classifier  +  ChiSquare  +  /content/DLBCL.csv \n",
            "\n",
            "Random Forest Classifier  +  ChiSquare  +  /content/DLBCL.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  ChiSquare  +  /content/DLBCL.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  ChiSquare  +  /content/DLBCL.csv \n",
            "\n",
            "K Nearest Neighbour  +  ChiSquare  +  /content/DLBCL.csv \n",
            "\n",
            "Support Vector Classifier  +  SVMRFE  +  /content/DLBCL.csv \n",
            "\n",
            "Decision Tree Classifier  +  SVMRFE  +  /content/DLBCL.csv \n",
            "\n",
            "Random Forest Classifier  +  SVMRFE  +  /content/DLBCL.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  SVMRFE  +  /content/DLBCL.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  SVMRFE  +  /content/DLBCL.csv \n",
            "\n",
            "K Nearest Neighbour  +  SVMRFE  +  /content/DLBCL.csv \n",
            "\n",
            "Support Vector Classifier  +  Variable ReliefF  +  /content/DLBCL.csv \n",
            "\n",
            "Decision Tree Classifier  +  Variable ReliefF  +  /content/DLBCL.csv \n",
            "\n",
            "Random Forest Classifier  +  Variable ReliefF  +  /content/DLBCL.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  Variable ReliefF  +  /content/DLBCL.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  Variable ReliefF  +  /content/DLBCL.csv \n",
            "\n",
            "K Nearest Neighbour  +  Variable ReliefF  +  /content/DLBCL.csv \n",
            "\n",
            "(126, 311)\n",
            "Support Vector Classifier  +  Original  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Decision Tree Classifier  +  Original  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Random Forest Classifier  +  Original  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  Original  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  Original  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "K Nearest Neighbour  +  Original  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Support Vector Classifier  +  ReliefF  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Decision Tree Classifier  +  ReliefF  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Random Forest Classifier  +  ReliefF  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  ReliefF  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  ReliefF  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "K Nearest Neighbour  +  ReliefF  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Support Vector Classifier  +  ChiSquare  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Decision Tree Classifier  +  ChiSquare  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Random Forest Classifier  +  ChiSquare  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  ChiSquare  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  ChiSquare  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "K Nearest Neighbour  +  ChiSquare  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Support Vector Classifier  +  SVMRFE  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Decision Tree Classifier  +  SVMRFE  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Random Forest Classifier  +  SVMRFE  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  SVMRFE  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  SVMRFE  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "K Nearest Neighbour  +  SVMRFE  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Support Vector Classifier  +  Variable ReliefF  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Decision Tree Classifier  +  Variable ReliefF  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Random Forest Classifier  +  Variable ReliefF  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  Variable ReliefF  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  Variable ReliefF  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "K Nearest Neighbour  +  Variable ReliefF  +  /content/LSVT_voice_rehabilitation.csv \n",
            "\n",
            "(30, 4523)\n",
            "Support Vector Classifier  +  Original  +  /content/gastric cancer.csv \n",
            "\n",
            "Decision Tree Classifier  +  Original  +  /content/gastric cancer.csv \n",
            "\n",
            "Random Forest Classifier  +  Original  +  /content/gastric cancer.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  Original  +  /content/gastric cancer.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  Original  +  /content/gastric cancer.csv \n",
            "\n",
            "K Nearest Neighbour  +  Original  +  /content/gastric cancer.csv \n",
            "\n",
            "Support Vector Classifier  +  ReliefF  +  /content/gastric cancer.csv \n",
            "\n",
            "Decision Tree Classifier  +  ReliefF  +  /content/gastric cancer.csv \n",
            "\n",
            "Random Forest Classifier  +  ReliefF  +  /content/gastric cancer.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  ReliefF  +  /content/gastric cancer.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  ReliefF  +  /content/gastric cancer.csv \n",
            "\n",
            "K Nearest Neighbour  +  ReliefF  +  /content/gastric cancer.csv \n",
            "\n",
            "Support Vector Classifier  +  ChiSquare  +  /content/gastric cancer.csv \n",
            "\n",
            "Decision Tree Classifier  +  ChiSquare  +  /content/gastric cancer.csv \n",
            "\n",
            "Random Forest Classifier  +  ChiSquare  +  /content/gastric cancer.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  ChiSquare  +  /content/gastric cancer.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  ChiSquare  +  /content/gastric cancer.csv \n",
            "\n",
            "K Nearest Neighbour  +  ChiSquare  +  /content/gastric cancer.csv \n",
            "\n",
            "Support Vector Classifier  +  SVMRFE  +  /content/gastric cancer.csv \n",
            "\n",
            "Decision Tree Classifier  +  SVMRFE  +  /content/gastric cancer.csv \n",
            "\n",
            "Random Forest Classifier  +  SVMRFE  +  /content/gastric cancer.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  SVMRFE  +  /content/gastric cancer.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  SVMRFE  +  /content/gastric cancer.csv \n",
            "\n",
            "K Nearest Neighbour  +  SVMRFE  +  /content/gastric cancer.csv \n",
            "\n",
            "Support Vector Classifier  +  Variable ReliefF  +  /content/gastric cancer.csv \n",
            "\n",
            "Decision Tree Classifier  +  Variable ReliefF  +  /content/gastric cancer.csv \n",
            "\n",
            "Random Forest Classifier  +  Variable ReliefF  +  /content/gastric cancer.csv \n",
            "\n",
            "Gaussian Naive Bayes  +  Variable ReliefF  +  /content/gastric cancer.csv \n",
            "\n",
            "Gradient Boosting Classifier  +  Variable ReliefF  +  /content/gastric cancer.csv \n",
            "\n",
            "K Nearest Neighbour  +  Variable ReliefF  +  /content/gastric cancer.csv \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores.to_csv('low_instance_binary.csv')"
      ],
      "metadata": {
        "id": "HdibIG27sLNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path_name_list = ['/content/gastroenterology.csv','/content/leukemia.csv', '/content/colon 2000.csv', '/content/DLBCL.csv', '/content/LSVT_voice_rehabilitation.csv', '/content/gastric cancer.csv',]\n",
        "path_name_list = ['/content/staDynBenignLab.csv','/content/qsar_androgen_receptor.csv','/content/qsar_oral_toxicity.csv']\n",
        "df_scores = pd.DataFrame(columns=['Algorithm','Dataset','Model','Fold Number', 'Accuracy','F1','Geometric Mean','AUC'])\n",
        "feature_names_path = 'featuresHigh.csv'\n",
        "count = 1\n",
        "for path_name in path_name_list:\n",
        "  main_function(path_name, df_scores, feature_names_path)\n",
        "  count = count + 1"
      ],
      "metadata": {
        "id": "ml7dcKzqzPlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores.to_csv('high_instance_binary.csv')"
      ],
      "metadata": {
        "id": "Q1MxGg60zQpt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}